"""
Voice embedding generation module.
ÐŸÐ¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÑ‚ OpenAI Whisper Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð¸ Ð°Ð»ÑŒÑ‚ÐµÑ€Ð½Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ðµ Ð¼ÐµÑ‚Ð¾Ð´Ñ‹.
"""
import numpy as np
from typing import Optional, Protocol
from abc import ABC, abstractmethod
import logging
import hashlib
import tempfile
import os

from app.config import settings
from app.core.pitch_extraction import PitchAnalysisResult
from app.core.timbre_extraction import timbre_extractor

logger = logging.getLogger(__name__)


# Embedding dimension (matches typical audio embedding dimensions)
EMBEDDING_DIM = 512


class VoiceEmbeddingProvider(Protocol):
    """Protocol for voice embedding providers."""
    
    def generate_embedding(
        self, 
        audio: np.ndarray, 
        sr: int
    ) -> np.ndarray:
        """Generate voice embedding from audio."""
        ...


class BaseEmbeddingProvider(ABC):
    """Base class for embedding providers."""
    
    @abstractmethod
    def generate_embedding(
        self, 
        audio: np.ndarray, 
        sr: int
    ) -> np.ndarray:
        """Generate voice embedding from audio."""
        pass
    
    @property
    @abstractmethod
    def embedding_dim(self) -> int:
        """Return embedding dimension."""
        pass


class OpenAIAudioEmbeddingProvider(BaseEmbeddingProvider):
    """
    OpenAI Audio API embedding provider.
    
    TODO: Implement when OpenAI Audio API becomes available.
    Currently returns placeholder embeddings.
    """
    
    def __init__(self, api_key: str = None):
        self.api_key = api_key or settings.openai_api_key
        self._embedding_dim = EMBEDDING_DIM
        
        if not self.api_key:
            logger.warning(
                "OpenAI API key not configured. Using placeholder embeddings."
            )
    
    def _build_proxy_url(self) -> str:
        """
        Ð¡Ñ‚Ñ€Ð¾Ð¸Ñ‚ URL Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð´Ð»Ñ OpenAI.
        
        Returns:
            URL Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð¸Ð»Ð¸ Ð¿ÑƒÑÑ‚Ð°Ñ ÑÑ‚Ñ€Ð¾ÐºÐ° ÐµÑÐ»Ð¸ Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½
        """
        if not settings.openai_proxy_host or not settings.openai_proxy_port:
            return ""
        
        proxy_type = settings.openai_proxy_type.lower()
        host = settings.openai_proxy_host
        port = settings.openai_proxy_port
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€ÑƒÐµÐ¼ URL Ð¿Ñ€Ð¾ÐºÑÐ¸
        if settings.openai_proxy_username and settings.openai_proxy_password:
            # ÐŸÑ€Ð¾ÐºÑÐ¸ Ñ Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸ÐµÐ¹
            username = settings.openai_proxy_username
            password = settings.openai_proxy_password
            
            if proxy_type == "socks5":
                return f"socks5://{username}:{password}@{host}:{port}"
            else:
                return f"http://{username}:{password}@{host}:{port}"
        else:
            # ÐŸÑ€Ð¾ÐºÑÐ¸ Ð±ÐµÐ· Ð°ÑƒÑ‚ÐµÐ½Ñ‚Ð¸Ñ„Ð¸ÐºÐ°Ñ†Ð¸Ð¸
            if proxy_type == "socks5":
                return f"socks5://{host}:{port}"
            else:
                return f"http://{host}:{port}"
    
    @property
    def embedding_dim(self) -> int:
        return self._embedding_dim
    
    def generate_embedding(
        self, 
        audio: np.ndarray, 
        sr: int
    ) -> np.ndarray:
        """
        Generate voice embedding using OpenAI Audio API.
        
        Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ OpenAI Whisper Ð´Ð»Ñ Ð¸Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ñ embeddings Ð¸Ð· Ð°ÑƒÐ´Ð¸Ð¾.
        Ð•ÑÐ»Ð¸ API Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ fallback Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ features.
        
        Args:
            audio: Audio array
            sr: Sample rate
            
        Returns:
            Voice embedding vector
        """
        if not self.api_key:
            logger.info("Generating placeholder embedding (OpenAI API not configured)")
            return self._generate_placeholder_embedding(audio, sr)
        
        try:
            from openai import OpenAI
            import soundfile as sf
            import librosa
            import httpx
            import os
            
            # ÐÐ°ÑÑ‚Ñ€Ð°Ð¸Ð²Ð°ÐµÐ¼ Ð¿Ñ€Ð¾ÐºÑÐ¸ Ñ‡ÐµÑ€ÐµÐ· httpx.AsyncHTTPTransport (Ð¿Ñ€Ð°Ð²Ð¸Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð± Ð´Ð»Ñ SOCKS5)
            http_client = None
            
            if settings.openai_proxy_host and settings.openai_proxy_port:
                proxy_url = self._build_proxy_url()
                if proxy_url:
                    logger.info(f"ðŸ”§ Configuring proxy: {settings.openai_proxy_type.upper()} {settings.openai_proxy_host}:{settings.openai_proxy_port}")
                    
                    # Ð”Ð»Ñ SOCKS5 Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ httpx-socks
                    if settings.openai_proxy_type.lower() == "socks5":
                        try:
                            # Ð˜Ð¼Ð¿Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ SyncProxyTransport Ð¸Ð· httpx_socks
                            from httpx_socks import SyncProxyTransport
                            
                            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ sync transport Ñ SOCKS5 Ð¿Ñ€Ð¾ÐºÑÐ¸ (OpenAI SDK ÑÐ¸Ð½Ñ…Ñ€Ð¾Ð½Ð½Ñ‹Ð¹)
                            transport = SyncProxyTransport.from_url(
                                proxy_url,
                                limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
                            )
                            
                            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ sync HTTP ÐºÐ»Ð¸ÐµÐ½Ñ‚ Ñ SOCKS5 transport
                            http_client = httpx.Client(
                                transport=transport,
                                timeout=httpx.Timeout(900.0, connect=30.0),  # 15 Ð¼Ð¸Ð½ÑƒÑ‚ Ð¾Ð±Ñ‰Ð¸Ð¹, 30 ÑÐµÐº Ð½Ð° Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ
                            )
                            
                            logger.info("âœ… SOCKS5 proxy configured via httpx_socks.SyncProxyTransport")
                            
                        except ImportError:
                            logger.error("âŒ httpx-socks not installed! Install with: pip install httpx-socks")
                            logger.warning("âš ï¸  Falling back to direct connection")
                            http_client = None
                        except Exception as e:
                            logger.error(f"âŒ Error configuring SOCKS5 proxy: {e}")
                            logger.warning("âš ï¸  Falling back to direct connection")
                            http_client = None
                    else:
                        # Ð”Ð»Ñ HTTP Ð¿Ñ€Ð¾ÐºÑÐ¸ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ ÑÑ‚Ð°Ð½Ð´Ð°Ñ€Ñ‚Ð½Ñ‹Ð¹ ÑÐ¿Ð¾ÑÐ¾Ð±
                        transport = httpx.HTTPTransport(
                            proxy=proxy_url,
                            limits=httpx.Limits(max_keepalive_connections=5, max_connections=10),
                        )
                        http_client = httpx.Client(
                            transport=transport,
                            timeout=httpx.Timeout(900.0, connect=30.0),
                        )
                        logger.info("âœ… HTTP proxy configured via httpx.HTTPTransport")
                else:
                    logger.info("ðŸ”§ No proxy configured, using direct connection")
            else:
                logger.info("ðŸ”§ No proxy configured, using direct connection")
            
            # Ð¡Ð¾Ð·Ð´Ð°ÐµÐ¼ ÐºÐ»Ð¸ÐµÐ½Ñ‚ OpenAI Ñ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½Ð½Ñ‹Ð¼ http_client
            if http_client:
                client = OpenAI(
                    api_key=self.api_key,
                    http_client=http_client,
                    timeout=900.0,  # 15 Ð¼Ð¸Ð½ÑƒÑ‚ Ð´Ð»Ñ Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¾ÐºÑÐ¸
                    max_retries=3,
                )
            else:
                # ÐŸÑ€ÑÐ¼Ð¾Ðµ Ð¿Ð¾Ð´ÐºÐ»ÑŽÑ‡ÐµÐ½Ð¸Ðµ Ð±ÐµÐ· Ð¿Ñ€Ð¾ÐºÑÐ¸
                client = OpenAI(
                    api_key=self.api_key,
                    timeout=900.0,
                    max_retries=3,
                )
            
            # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð°ÑƒÐ´Ð¸Ð¾ Ð²Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð» (OpenAI Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ Ñ„Ð°Ð¹Ð»)
            tmp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)
            tmp_path = tmp_file.name
            tmp_file.close()
            
            try:
                    # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼ sample rate Ð´Ð¾ 16kHz (Ñ‚Ñ€ÐµÐ±Ð¾Ð²Ð°Ð½Ð¸Ðµ Whisper)
                    target_sr = 16000
                    if sr != target_sr:
                        audio_resampled = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
                    else:
                        audio_resampled = audio
                    
                    # ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡Ð¸Ð²Ð°ÐµÐ¼ Ð´Ð»Ð¸Ð½Ñƒ Ð´Ð¾ 45 ÑÐµÐºÑƒÐ½Ð´ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ñ‡ÐµÑ€ÐµÐ· Ð¼ÐµÐ´Ð»ÐµÐ½Ð½Ñ‹Ð¹ Ð¿Ñ€Ð¾ÐºÑÐ¸
                    max_samples = 45 * target_sr  # 45 ÑÐµÐºÑƒÐ½Ð´
                    if len(audio_resampled) > max_samples:
                        logger.info(f"Truncating audio from {len(audio_resampled)/target_sr:.1f}s to 45s for faster processing")
                        audio_resampled = audio_resampled[:max_samples]
                    
                    # Ð¡Ð¾Ñ…Ñ€Ð°Ð½ÑÐµÐ¼ Ð² WAV
                    sf.write(tmp_path, audio_resampled, target_sr)
                    
                    # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Whisper Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                    # Whisper Ð¿Ð¾Ð¼Ð¾Ð³Ð°ÐµÑ‚ Ð»ÑƒÑ‡ÑˆÐµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð²Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸
                    with open(tmp_path, 'rb') as audio_file:
                        try:
                            logger.info("Calling OpenAI Whisper API...")
                            # ÐŸÐ¾Ð»ÑƒÑ‡Ð°ÐµÐ¼ Ñ‚Ñ€Ð°Ð½ÑÐºÑ€Ð¸Ð¿Ñ†Ð¸ÑŽ Ð´Ð»Ñ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
                            transcript = client.audio.transcriptions.create(
                                model="whisper-1",
                                file=audio_file,
                                response_format="verbose_json",
                            )
                            
                            # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Whisper Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ embedding
                            # Whisper Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ, Ñ‡Ñ‚Ð¾ ÑƒÐ»ÑƒÑ‡ÑˆÐ°ÐµÑ‚ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ð¾
                            logger.info("OpenAI Whisper analysis completed, enhancing embedding")
                            
                            # Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÐ¼ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ embedding Ñ ÑƒÑ‡ÐµÑ‚Ð¾Ð¼ Whisper Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
                            # TranscriptionVerbose Ð¸Ð¼ÐµÐµÑ‚ Ð¿Ð¾Ð»Ñ: text, language, duration, segments, words
                            transcript_dict = {
                                "text": transcript.text,
                                "language": transcript.language,
                                "duration": transcript.duration,
                                "has_speech": len(transcript.text.strip()) > 0,
                            }
                            enhanced_embedding = self._generate_enhanced_embedding(
                                audio, sr, transcript_dict
                            )
                            return enhanced_embedding
                            
                        except Exception as e:
                            error_msg = str(e)
                            error_type = type(e).__name__
                            logger.error(f"OpenAI Whisper API error ({error_type}): {error_msg}")
                            
                            # ÐŸÑ€Ð¾Ð²ÐµÑ€ÑÐµÐ¼ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ‚Ð¸Ð¿Ñ‹ Ð¾ÑˆÐ¸Ð±Ð¾Ðº
                            if "insufficient_quota" in error_msg.lower():
                                logger.error("âŒ OpenAI quota exceeded! Please add credits at https://platform.openai.com/account/billing")
                            elif "authentication" in error_msg.lower() or "api_key" in error_msg.lower():
                                logger.error("âŒ OpenAI API key invalid! Check OPENAI_API_KEY in .env")
                            elif "timeout" in error_msg.lower() or "timed out" in error_msg.lower():
                                logger.error("âŒ Request timeout. Try: 1) Shorter audio 2) Check internet 3) Disable proxy")
                            elif "connection" in error_msg.lower():
                                logger.error("âŒ Connection error. Try: 1) Check internet 2) Try without proxy 3) Check firewall")
                            
                            logger.warning("Using placeholder embedding as fallback")
                            return self._generate_placeholder_embedding(audio, sr)
            finally:
                # Ð£Ð´Ð°Ð»ÑÐµÐ¼ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ð¹ Ñ„Ð°Ð¹Ð»
                if os.path.exists(tmp_path):
                    os.unlink(tmp_path)
                
                # Ð—Ð°ÐºÑ€Ñ‹Ð²Ð°ÐµÐ¼ http_client ÐµÑÐ»Ð¸ Ð±Ñ‹Ð» ÑÐ¾Ð·Ð´Ð°Ð½ (sync ÐºÐ»Ð¸ÐµÐ½Ñ‚)
                if http_client:
                    try:
                        # httpx.Client Ð½ÑƒÐ¶Ð½Ð¾ Ð·Ð°ÐºÑ€Ñ‹Ð²Ð°Ñ‚ÑŒ Ñ‡ÐµÑ€ÐµÐ· close()
                        http_client.close()
                    except Exception as e:
                        logger.debug(f"Error closing http_client: {e}")
                        
        except ImportError:
            logger.warning("OpenAI library not installed. Install with: pip install openai")
            return self._generate_placeholder_embedding(audio, sr)
        except Exception as e:
            logger.error(f"Error calling OpenAI API: {e}. Using fallback.")
            return self._generate_placeholder_embedding(audio, sr)
    
    def _generate_enhanced_embedding(
        self,
        audio: np.ndarray,
        sr: int,
        whisper_transcript: dict
    ) -> np.ndarray:
        """
        Ð“ÐµÐ½ÐµÑ€Ð¸Ñ€ÑƒÐµÑ‚ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð½Ñ‹Ð¹ embedding Ñ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸ÐµÐ¼ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð¾Ñ‚ Whisper.
        
        Whisper Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð°ÑƒÐ´Ð¸Ð¾ Ð½Ð° Ð³Ð»ÑƒÐ±Ð¾ÐºÐ¾Ð¼ ÑƒÑ€Ð¾Ð²Ð½Ðµ, Ñ‡Ñ‚Ð¾ Ð¿Ð¾Ð·Ð²Ð¾Ð»ÑÐµÑ‚
        Ð»ÑƒÑ‡ÑˆÐµ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ñ‚ÑŒ Ð²Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸ Ð¸ Ñ‚ÐµÐ¼Ð±Ñ€.
        """
        # Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÐ¼ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ðµ features
        base_embedding = self._generate_placeholder_embedding(audio, sr)
        
        # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Whisper Ð´Ð»Ñ ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ñ
        # ÐÐ°Ð¿Ñ€Ð¸Ð¼ÐµÑ€, Ð´Ð»Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾ÑÑ‚ÑŒ, ÑÐ·Ñ‹Ðº, ÑƒÐ²ÐµÑ€ÐµÐ½Ð½Ð¾ÑÑ‚ÑŒ Ð¼Ð¾Ð´ÐµÐ»Ð¸
        whisper_features = np.array([
            whisper_transcript.get('duration', 0) / 100.0,  # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼
            len(whisper_transcript.get('text', '')) / 1000.0,  # Ð”Ð»Ð¸Ð½Ð° Ñ‚ÐµÐºÑÑ‚Ð°
            1.0 if whisper_transcript.get('text') else 0.0,  # Ð•ÑÑ‚ÑŒ Ð»Ð¸ Ñ€ÐµÑ‡ÑŒ
        ], dtype=np.float32)
        
        # ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€ÑƒÐµÐ¼ Ñ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¼ embedding
        # Ð Ð°ÑÑˆÐ¸Ñ€ÑÐµÐ¼ whisper_features Ð´Ð¾ Ð½ÑƒÐ¶Ð½Ð¾Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸
        whisper_expanded = self._expand_features(whisper_features, self._embedding_dim)
        
        # Ð’Ð·Ð²ÐµÑˆÐµÐ½Ð½Ð°Ñ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð°Ñ†Ð¸Ñ (70% Ð±Ð°Ð·Ð¾Ð²Ñ‹Ð¹, 30% Whisper enhancement)
        enhanced = 0.7 * base_embedding + 0.3 * whisper_expanded
        
        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·ÑƒÐµÐ¼
        norm = np.linalg.norm(enhanced)
        if norm > 0:
            enhanced = enhanced / norm
        
        return enhanced.astype(np.float32)
    
    def _generate_placeholder_embedding(
        self, 
        audio: np.ndarray, 
        sr: int
    ) -> np.ndarray:
        """
        Generate placeholder embedding based on audio features.
        
        This is a fallback that creates reproducible embeddings
        from OpenSMILE features until real API is available.
        """
        # Extract timbre features
        features = timbre_extractor.extract_features(audio, sr)
        feature_vector = timbre_extractor.features_to_vector(features)
        
        # Expand to full embedding dimension with hashing
        # This ensures reproducible embeddings from the same audio
        expanded = self._expand_features(feature_vector, self._embedding_dim)
        
        # Normalize
        norm = np.linalg.norm(expanded)
        if norm > 0:
            expanded = expanded / norm
        
        return expanded.astype(np.float32)
    
    def _expand_features(
        self, 
        features: np.ndarray, 
        target_dim: int
    ) -> np.ndarray:
        """Expand feature vector to target dimension."""
        if len(features) >= target_dim:
            return features[:target_dim]
        
        # Create expanded vector by repeating and transforming features
        expanded = np.zeros(target_dim, dtype=np.float32)
        
        # Copy original features
        expanded[:len(features)] = features
        
        # Fill remaining with transformed features
        np.random.seed(42)  # Reproducible
        transform_matrix = np.random.randn(len(features), target_dim - len(features))
        expanded[len(features):] = features @ transform_matrix
        
        return expanded


class LocalFeatureEmbeddingProvider(BaseEmbeddingProvider):
    """
    Local embedding provider using extracted audio features.
    Creates embeddings from pitch and timbre features without external API.
    """
    
    def __init__(self, embedding_dim: int = EMBEDDING_DIM):
        self._embedding_dim = embedding_dim
    
    @property
    def embedding_dim(self) -> int:
        return self._embedding_dim
    
    def generate_embedding(
        self, 
        audio: np.ndarray, 
        sr: int,
        pitch_analysis: Optional[PitchAnalysisResult] = None,
    ) -> np.ndarray:
        """
        Generate embedding from local audio features.
        
        Args:
            audio: Audio array
            sr: Sample rate
            pitch_analysis: Optional pre-computed pitch analysis
            
        Returns:
            Feature-based embedding vector
        """
        logger.info("Generating local feature-based embedding")
        
        # Extract timbre features
        timbre_features = timbre_extractor.extract_features(audio, sr)
        timbre_vector = timbre_extractor.features_to_vector(timbre_features)
        
        # Normalize timbre features
        timbre_norm = np.linalg.norm(timbre_vector)
        if timbre_norm > 0:
            timbre_vector = timbre_vector / timbre_norm
        
        # Add pitch features if available
        if pitch_analysis:
            pitch_features = np.array([
                pitch_analysis.min_pitch_hz / 1000,  # Normalize to ~0-1 range
                pitch_analysis.max_pitch_hz / 1000,
                pitch_analysis.median_pitch_hz / 1000,
                pitch_analysis.octave_range / 4,  # Normalize assuming max 4 octaves
                pitch_analysis.voiced_ratio,
            ], dtype=np.float32)
        else:
            pitch_features = np.zeros(5, dtype=np.float32)
        
        # Concatenate features
        combined = np.concatenate([timbre_vector, pitch_features])
        
        # Expand to embedding dimension
        embedding = self._project_to_embedding(combined, self._embedding_dim)
        
        # Normalize final embedding
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding.astype(np.float32)
    
    def _project_to_embedding(
        self, 
        features: np.ndarray, 
        target_dim: int
    ) -> np.ndarray:
        """Project features to target embedding dimension."""
        n_features = len(features)
        
        if n_features >= target_dim:
            return features[:target_dim]
        
        # Use random projection (reproducible)
        np.random.seed(42)
        projection_matrix = np.random.randn(n_features, target_dim).astype(np.float32)
        projection_matrix /= np.sqrt(n_features)  # Scale for unit variance
        
        return features @ projection_matrix


class VoiceEmbeddingGenerator:
    """
    Main voice embedding generator that selects appropriate provider.
    """
    
    def __init__(self, provider: str = "auto"):
        """
        Initialize embedding generator.
        
        Args:
            provider: Provider type ("openai", "local", "auto")
                - "openai": Ð’ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ OpenAI
                - "local": Ð’ÑÐµÐ³Ð´Ð° Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÑ‚ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹
                - "auto": Ð’Ñ‹Ð±Ð¸Ñ€Ð°ÐµÑ‚ Ð°Ð²Ñ‚Ð¾Ð¼Ð°Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
        """
        from app.config import settings
        
        if provider == "openai":
            self.provider = OpenAIAudioEmbeddingProvider()
        elif provider == "local":
            self.provider = LocalFeatureEmbeddingProvider()
        else:
            # "auto" - Ð²Ñ‹Ð±Ð¸Ñ€Ð°ÐµÐ¼ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐº
            if settings.use_openai_for_user_analysis and settings.openai_api_key:
                # ÐŸÑ‹Ñ‚Ð°ÐµÐ¼ÑÑ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÑŒ OpenAI ÐµÑÐ»Ð¸ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½
                try:
                    self.provider = OpenAIAudioEmbeddingProvider()
                    print(f"[EMBEDDING] Using provider: {type(self.provider).__name__} (OpenAI enabled)")
                except Exception as e:
                    print(f"[EMBEDDING] OpenAI provider failed: {e}, falling back to local")
                    self.provider = LocalFeatureEmbeddingProvider()
            else:
                # Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð»Ð¾ÐºÐ°Ð»ÑŒÐ½Ñ‹Ð¹ ÐµÑÐ»Ð¸ OpenAI Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½ Ð¸Ð»Ð¸ Ð¾Ñ‚ÐºÐ»ÑŽÑ‡ÐµÐ½
                self.provider = LocalFeatureEmbeddingProvider()
                if not settings.openai_api_key:
                    print(f"[EMBEDDING] Using provider: {type(self.provider).__name__} (OpenAI not configured)")
                else:
                    print(f"[EMBEDDING] Using provider: {type(self.provider).__name__} (OpenAI disabled for user analysis)")
        
        if provider != "auto":
            print(f"[EMBEDDING] Using provider: {type(self.provider).__name__}")
    
    def generate(
        self, 
        audio: np.ndarray, 
        sr: int,
        pitch_analysis: Optional[PitchAnalysisResult] = None,
    ) -> np.ndarray:
        """
        Generate voice embedding.
        
        Args:
            audio: Audio array
            sr: Sample rate
            pitch_analysis: Optional pitch analysis result
            
        Returns:
            Voice embedding vector
        """
        if isinstance(self.provider, LocalFeatureEmbeddingProvider):
            return self.provider.generate_embedding(audio, sr, pitch_analysis)
        else:
            return self.provider.generate_embedding(audio, sr)
    
    @property
    def embedding_dim(self) -> int:
        return self.provider.embedding_dim


# Module-level instance
embedding_generator = VoiceEmbeddingGenerator()
